{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_vBZKuZ6CqBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyinrUfSGQUR"
      },
      "outputs": [],
      "source": [
        "# DECISION TREE CLASSIFIER WITHOUT BALANCING THE DATASET\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import gridspec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NVfoK7GUMiU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU0SgWlCGr7o"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load the dataset\n",
        "data =pd.read_csv(\"/content/drive/MyDrive/Major Projecct/creditcard.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKRWX2kuG2yM"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiBKraOSHVQB"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = data[data.duplicated()]\n",
        "print(duplicates)\n",
        "#Removing the Duplicate Values\n",
        "data = data.drop_duplicates()\n",
        "print(data[data.duplicated()])"
      ],
      "metadata": {
        "id": "R9tCSnYTCdf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r15gsQ0NHlH_"
      },
      "outputs": [],
      "source": [
        "#checking for NaN values in the dataset\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXQ6Ue2LL7iv"
      },
      "outputs": [],
      "source": [
        "data.fillna(data.mean(), inplace=True)\n",
        "# Separate features and target variable\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "y.fillna(1, inplace=True)\n",
        "\n",
        "# Perform label encoding for categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_cols = X.columns\n",
        "for col in categorical_cols:\n",
        "    X[col] = label_encoder.fit_transform(X[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUqhajQzMOgh"
      },
      "outputs": [],
      "source": [
        "#checking if the NaN values have been replaced or not.\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUKXVbb6Mh3f"
      },
      "outputs": [],
      "source": [
        "#Splitting the data in to train and target variables\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0QyKABFOFw1"
      },
      "outputs": [],
      "source": [
        "#Split the dataset into training and testing sets\n",
        "for i in range(1, 5):\n",
        "  r = random.randint(1,100)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=r)\n",
        "  scaler = RobustScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "  # Initialize the XGBoost classifier\n",
        "  xgb_model = XGBClassifier()\n",
        "\n",
        "  # Fit the model on the training data\n",
        "  xgb_model.fit(X_train, y_train)\n",
        "\n",
        "  # Predict on the test data\n",
        "  y_pred = xgb_model.predict(X_test)\n",
        "  # Model Accuracy, how often is the classifier correct?\n",
        "  print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
        "  f1 = f1_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  recall = recall_score(y_test, y_pred)\n",
        "  confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  print(\"F1 Score:\", f1)\n",
        "  print(\"Precision Score:\", precision)\n",
        "  print(\"Recall Score:\", recall)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luv4bNoGPMEA"
      },
      "outputs": [],
      "source": [
        "#visulalizing the confusion matrix\n",
        "LABELS = ['Normal', 'Fraud']\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize =(12, 12))\n",
        "sns.heatmap(conf_matrix, xticklabels = LABELS, yticklabels = LABELS, annot = True, fmt =\"d\");\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}